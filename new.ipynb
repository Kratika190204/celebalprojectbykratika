{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c196207-c858-4969-8a2e-3d9ecf09e59d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-26 16:58:44,837 - INFO - CLV Predictor initialized\n",
      "2025-06-26 16:58:44,841 - INFO - Loading data from OnlineRetail.xlsx\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current directory: C:\\Users\\krati\\celebal Project\n",
      "Files in current directory: ['.ipynb_checkpoints', 'app.py', 'new.ipynb', 'OnlineRetail.xlsx', 'simple_test.py', 'Untitled.ipynb']\n",
      "Attempting to load: OnlineRetail.xlsx\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-26 16:59:22,788 - INFO - Successfully loaded Excel file: OnlineRetail.xlsx\n",
      "2025-06-26 16:59:22,790 - INFO - Original dataset shape: (525461, 8)\n",
      "2025-06-26 16:59:22,790 - INFO - Columns: ['Invoice', 'StockCode', 'Description', 'Quantity', 'InvoiceDate', 'Price', 'Customer ID', 'Country']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available columns: ['Invoice', 'StockCode', 'Description', 'Quantity', 'InvoiceDate', 'Price', 'Customer ID', 'Country']\n",
      "Column mapping: {'StockCode': 'StockCode', 'Description': 'Description', 'Quantity': 'Quantity', 'InvoiceDate': 'InvoiceDate', 'Price': 'UnitPrice', 'Customer ID': 'CustomerID', 'Country': 'Country'}\n",
      "Columns after mapping: ['Invoice', 'StockCode', 'Description', 'Quantity', 'InvoiceDate', 'UnitPrice', 'CustomerID', 'Country']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-26 16:59:22,878 - INFO - Removed 107927 rows with missing CustomerID\n",
      "2025-06-26 16:59:23,435 - INFO - Data date range: 2009-12-01 07:45:00 to 2010-12-09 20:01:00\n",
      "2025-06-26 16:59:23,467 - INFO - Number of unique customers: 4304\n",
      "2025-06-26 16:59:23,478 - INFO - Cleaned dataset shape: (407259, 16)\n",
      "2025-06-26 16:59:23,480 - INFO - Creating temporal split to prevent data leakage\n",
      "2025-06-26 16:59:23,564 - INFO - Feature period: 2009-06-17 20:01:00 to 2010-06-12 20:01:00\n",
      "2025-06-26 16:59:23,565 - INFO - Prediction period: 2010-06-12 20:01:00 to 2010-12-09 20:01:00\n",
      "2025-06-26 16:59:23,852 - INFO - Feature data shape: (176904, 16)\n",
      "2025-06-26 16:59:23,854 - INFO - Target data shape: (230355, 16)\n",
      "2025-06-26 16:59:23,870 - INFO - Preparing target variable\n",
      "2025-06-26 16:59:23,934 - INFO - Creating advanced features\n",
      "2025-06-26 16:59:24,995 - INFO - Created features for 2815 customers\n",
      "2025-06-26 16:59:25,010 - INFO - Customers with future purchases: 1992\n",
      "2025-06-26 16:59:25,012 - INFO - Customers with no future purchases: 823\n",
      "2025-06-26 16:59:25,053 - INFO - Feature matrix shape: (2815, 35)\n",
      "2025-06-26 16:59:25,054 - INFO - Target variable shape: (2815,)\n",
      "2025-06-26 16:59:25,071 - INFO - Training and evaluating models with hyperparameter tuning\n",
      "2025-06-26 16:59:25,073 - INFO - Training Linear_Regression...\n",
      "2025-06-26 16:59:25,169 - INFO - Completed Linear_Regression - R2: 0.6909\n",
      "2025-06-26 16:59:25,171 - INFO - Training Ridge_Regression...\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime, timedelta\n",
    "import warnings\n",
    "import pickle\n",
    "import joblib\n",
    "from scipy.stats import spearmanr\n",
    "from scipy import stats\n",
    "import logging\n",
    "import os\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Machine Learning libraries\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV, TimeSeriesSplit\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder, RobustScaler\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, VotingRegressor, ExtraTreesRegressor\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error, mean_absolute_percentage_error\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.inspection import permutation_importance\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "\n",
    "from clv_predictor import EnhancedCLVPredictor\n",
    "\n",
    "# Additional libraries\n",
    "try:\n",
    "    import openpyxl\n",
    "except ImportError:\n",
    "    print(\"Warning: openpyxl not installed. Installing...\")\n",
    "    import subprocess\n",
    "    subprocess.check_call(['pip', 'install', 'openpyxl'])\n",
    "    import openpyxl\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "class CLVConfig:\n",
    "    \"\"\"Configuration class for CLV model parameters\"\"\"\n",
    "    \n",
    "    # Data cleaning parameters\n",
    "    MIN_PURCHASE_AMOUNT = 0\n",
    "    MIN_QUANTITY = 0\n",
    "    \n",
    "    # Feature engineering parameters\n",
    "    TRAINING_PERIOD_RATIO = 0.8\n",
    "    RFM_QUANTILES = 5\n",
    "    LOOKBACK_MONTHS = 12  # Months to look back for features\n",
    "    PREDICTION_MONTHS = 6  # Months to predict forward\n",
    "    \n",
    "    # Model training parameters\n",
    "    TEST_SIZE = 0.2\n",
    "    RANDOM_STATE = 42\n",
    "    CV_FOLDS = 5\n",
    "    N_JOBS = -1\n",
    "    \n",
    "    # Business parameters\n",
    "    HIGH_VALUE_THRESHOLD_PERCENTILE = 90\n",
    "    CHURN_RISK_MULTIPLIER = 2.0\n",
    "\n",
    "class EnhancedCLVPredictor:\n",
    "    \"\"\"Production-ready CLV prediction model with advanced features\"\"\"\n",
    "    \n",
    "    def __init__(self, config=None):\n",
    "        self.config = config or CLVConfig()\n",
    "        self.models = {}\n",
    "        self.best_model = None\n",
    "        self.scaler = None\n",
    "        self.feature_names = []\n",
    "        self.model_performance = {}\n",
    "        self.is_fitted = False\n",
    "        \n",
    "        logging.info(\"CLV Predictor initialized\")\n",
    "    \n",
    "    def load_and_clean_data(self, file_path):\n",
    "        \"\"\"Load and clean the retail dataset with enhanced error handling\"\"\"\n",
    "        logging.info(f\"Loading data from {file_path}\")\n",
    "        \n",
    "        try:\n",
    "            # Check current directory\n",
    "            current_dir = os.getcwd()\n",
    "            print(f\"Current directory: {current_dir}\")\n",
    "            print(f\"Files in current directory: {os.listdir('.')}\")\n",
    "            \n",
    "            # Try different possible filenames\n",
    "            possible_files = [\n",
    "                file_path,\n",
    "                file_path.replace(' ', '_'),\n",
    "                file_path.replace(' ', ''),\n",
    "                'online_retail.xlsx',\n",
    "                'online retail.xlsx',\n",
    "                'OnlineRetail.xlsx',\n",
    "                'Online Retail.xlsx'\n",
    "            ]\n",
    "            \n",
    "            df = None\n",
    "            loaded_file = None\n",
    "            \n",
    "            for filename in possible_files:\n",
    "                if os.path.exists(filename):\n",
    "                    loaded_file = filename\n",
    "                    break\n",
    "            \n",
    "            if loaded_file is None:\n",
    "                # List all Excel files in directory\n",
    "                excel_files = [f for f in os.listdir('.') if f.endswith(('.xlsx', '.xls'))]\n",
    "                print(f\"Available Excel files: {excel_files}\")\n",
    "                \n",
    "                if excel_files:\n",
    "                    print(f\"Using the first available Excel file: {excel_files[0]}\")\n",
    "                    loaded_file = excel_files[0]\n",
    "                else:\n",
    "                    raise FileNotFoundError(\"No Excel files found in the current directory\")\n",
    "            \n",
    "            # Try loading the file\n",
    "            print(f\"Attempting to load: {loaded_file}\")\n",
    "            \n",
    "            if loaded_file.endswith('.xlsx') or loaded_file.endswith('.xls'):\n",
    "                try:\n",
    "                    # Try reading Excel file\n",
    "                    df = pd.read_excel(loaded_file)\n",
    "                    logging.info(f\"Successfully loaded Excel file: {loaded_file}\")\n",
    "                except Exception as e:\n",
    "                    print(f\"Error reading Excel file: {e}\")\n",
    "                    # Try reading as CSV\n",
    "                    csv_file = loaded_file.replace('.xlsx', '.csv').replace('.xls', '.csv')\n",
    "                    if os.path.exists(csv_file):\n",
    "                        df = pd.read_csv(csv_file)\n",
    "                        logging.info(f\"Loaded CSV version: {csv_file}\")\n",
    "                    else:\n",
    "                        raise\n",
    "            else:\n",
    "                df = pd.read_csv(loaded_file)\n",
    "            \n",
    "            if df is None:\n",
    "                raise ValueError(\"Failed to load any data file\")\n",
    "            \n",
    "            logging.info(f\"Original dataset shape: {df.shape}\")\n",
    "            logging.info(f\"Columns: {list(df.columns)}\")\n",
    "            \n",
    "            # Data cleaning\n",
    "            df = self._clean_data(df)\n",
    "            logging.info(f\"Cleaned dataset shape: {df.shape}\")\n",
    "            \n",
    "            return df\n",
    "            \n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error loading data: {e}\")\n",
    "            print(f\"Error details: {str(e)}\")\n",
    "            raise\n",
    "    \n",
    "   \n",
    "    \n",
    "    def create_temporal_split(self, df):\n",
    "        \"\"\"Create proper temporal split to prevent data leakage\"\"\"\n",
    "        logging.info(\"Creating temporal split to prevent data leakage\")\n",
    "        \n",
    "        # Sort by date\n",
    "        df_sorted = df.sort_values('InvoiceDate')\n",
    "        \n",
    "        # Calculate split date based on configuration\n",
    "        min_date = df_sorted['InvoiceDate'].min()\n",
    "        max_date = df_sorted['InvoiceDate'].max()\n",
    "        total_days = (max_date - min_date).days\n",
    "        \n",
    "        # Use lookback period for features and prediction period for targets\n",
    "        feature_end_date = max_date - timedelta(days=self.config.PREDICTION_MONTHS * 30)\n",
    "        feature_start_date = feature_end_date - timedelta(days=self.config.LOOKBACK_MONTHS * 30)\n",
    "        \n",
    "        logging.info(f\"Feature period: {feature_start_date} to {feature_end_date}\")\n",
    "        logging.info(f\"Prediction period: {feature_end_date} to {max_date}\")\n",
    "        \n",
    "        # Split data\n",
    "        feature_data = df_sorted[\n",
    "            (df_sorted['InvoiceDate'] >= feature_start_date) & \n",
    "            (df_sorted['InvoiceDate'] <= feature_end_date)\n",
    "        ]\n",
    "        \n",
    "        target_data = df_sorted[\n",
    "            df_sorted['InvoiceDate'] > feature_end_date\n",
    "        ]\n",
    "        \n",
    "        logging.info(f\"Feature data shape: {feature_data.shape}\")\n",
    "        logging.info(f\"Target data shape: {target_data.shape}\")\n",
    "        \n",
    "        return feature_data, target_data\n",
    "        \n",
    "    def _clean_data(self, df):\n",
    "        \"\"\"Enhanced data cleaning with better validation\"\"\"\n",
    "    \n",
    "        # Print column names for debugging\n",
    "        print(f\"Available columns: {list(df.columns)}\")\n",
    "    \n",
    "        # Try to identify correct column names (case-insensitive and space-insensitive)\n",
    "        column_mapping = {}\n",
    "        for col in df.columns:\n",
    "            col_lower = str(col).lower().strip().replace(' ', '').replace('_', '')\n",
    "        \n",
    "            # More flexible column mapping\n",
    "            if 'customer' in col_lower and 'id' in col_lower:\n",
    "                column_mapping[col] = 'CustomerID'\n",
    "            elif 'invoice' in col_lower and 'date' in col_lower:\n",
    "                column_mapping[col] = 'InvoiceDate'  \n",
    "            elif col_lower == 'quantity':\n",
    "                column_mapping[col] = 'Quantity'\n",
    "            elif ('unit' in col_lower and 'price' in col_lower) or col_lower == 'price':\n",
    "                column_mapping[col] = 'UnitPrice'\n",
    "            elif 'invoice' in col_lower and ('no' in col_lower or 'number' in col_lower):\n",
    "                column_mapping[col] = 'InvoiceNo'\n",
    "            elif 'stock' in col_lower and 'code' in col_lower:\n",
    "                column_mapping[col] = 'StockCode'\n",
    "            elif 'description' in col_lower:\n",
    "                column_mapping[col] = 'Description'\n",
    "            elif col_lower == 'country':\n",
    "                column_mapping[col] = 'Country'\n",
    "    \n",
    "        # Apply column mapping\n",
    "        if column_mapping:\n",
    "            print(f\"Column mapping: {column_mapping}\")\n",
    "            df = df.rename(columns=column_mapping)\n",
    "            print(f\"Columns after mapping: {list(df.columns)}\")\n",
    "    \n",
    "        # Check required columns exist\n",
    "        required_columns = ['CustomerID', 'InvoiceDate', 'Quantity', 'UnitPrice']\n",
    "        missing_columns = [col for col in required_columns if col not in df.columns]\n",
    "    \n",
    "        if missing_columns:\n",
    "            logging.error(f\"Missing required columns: {missing_columns}\")\n",
    "            logging.info(f\"Available columns: {list(df.columns)}\")\n",
    "        \n",
    "            # Try to suggest similar columns\n",
    "            for missing_col in missing_columns:\n",
    "                similar_cols = [col for col in df.columns if missing_col.lower() in col.lower()]\n",
    "                if similar_cols:\n",
    "                    print(f\"Similar columns for {missing_col}: {similar_cols}\")\n",
    "        \n",
    "            raise ValueError(f\"Missing required columns: {missing_columns}\")\n",
    "    \n",
    "        # Remove rows with missing CustomerID\n",
    "        initial_rows = len(df)\n",
    "        df = df.dropna(subset=['CustomerID'])\n",
    "        logging.info(f\"Removed {initial_rows - len(df)} rows with missing CustomerID\")\n",
    "        \n",
    "        # Remove negative quantities and unit prices (returns and adjustments)\n",
    "        df = df[(df['Quantity'] > self.config.MIN_QUANTITY) & \n",
    "                (df['UnitPrice'] > self.config.MIN_PURCHASE_AMOUNT)]\n",
    "    \n",
    "        # Convert data types\n",
    "        df['InvoiceDate'] = pd.to_datetime(df['InvoiceDate'], errors='coerce')\n",
    "        df = df.dropna(subset=['InvoiceDate'])  # Remove rows with invalid dates\n",
    "    \n",
    "        df['CustomerID'] = df['CustomerID'].astype(str)\n",
    "    \n",
    "        # Create TotalAmount column\n",
    "        df['TotalAmount'] = df['Quantity'] * df['UnitPrice']\n",
    "    \n",
    "        # Remove outliers (transactions > 99.9th percentile)\n",
    "        upper_limit = df['TotalAmount'].quantile(0.999)\n",
    "        df = df[df['TotalAmount'] <= upper_limit]\n",
    "    \n",
    "        # Extract enhanced date components\n",
    "        df['Year'] = df['InvoiceDate'].dt.year\n",
    "        df['Month'] = df['InvoiceDate'].dt.month\n",
    "        df['Quarter'] = df['InvoiceDate'].dt.quarter\n",
    "        df['DayOfWeek'] = df['InvoiceDate'].dt.day_of_week\n",
    "        df['Hour'] = df['InvoiceDate'].dt.hour\n",
    "        df['IsWeekend'] = df['DayOfWeek'].isin([5, 6]).astype(int)\n",
    "    \n",
    "        # Add seasonality\n",
    "        df['Season'] = df['Month'].map({12: 'Winter', 1: 'Winter', 2: 'Winter',\n",
    "                                       3: 'Spring', 4: 'Spring', 5: 'Spring',\n",
    "                                       6: 'Summer', 7: 'Summer', 8: 'Summer',\n",
    "                                       9: 'Fall', 10: 'Fall', 11: 'Fall'})\n",
    "    \n",
    "        logging.info(f\"Data date range: {df['InvoiceDate'].min()} to {df['InvoiceDate'].max()}\")\n",
    "        logging.info(f\"Number of unique customers: {df['CustomerID'].nunique()}\")\n",
    "    \n",
    "        return df\n",
    "\n",
    "    def create_advanced_features(self, df, reference_date=None):\n",
    "        \"\"\"Create comprehensive features for CLV prediction\"\"\"\n",
    "        logging.info(\"Creating advanced features\")\n",
    "        \n",
    "        if reference_date is None:\n",
    "            reference_date = df['InvoiceDate'].max()\n",
    "        \n",
    "        # Basic aggregations\n",
    "        customer_features = df.groupby('CustomerID').agg({\n",
    "            'InvoiceDate': ['min', 'max', 'count'],\n",
    "            'Invoice': 'nunique',\n",
    "            'TotalAmount': ['sum', 'mean', 'std', 'min', 'max', 'median'],\n",
    "            'Quantity': ['sum', 'mean', 'std'],\n",
    "            'StockCode': 'nunique',\n",
    "            'UnitPrice': ['mean', 'std'],\n",
    "            'Year': 'nunique',\n",
    "            'Month': 'nunique',\n",
    "            'Quarter': 'nunique',\n",
    "            'DayOfWeek': lambda x: x.mode().iloc[0] if not x.empty else 0,\n",
    "            'Hour': lambda x: x.mode().iloc[0] if not x.empty else 0,\n",
    "            'IsWeekend': 'mean',\n",
    "            'Season': lambda x: x.mode().iloc[0] if not x.empty else 'Spring'\n",
    "        }).reset_index()\n",
    "        \n",
    "        # Flatten column names\n",
    "        customer_features.columns = [\n",
    "            'CustomerID', 'FirstPurchase', 'LastPurchase', 'TotalTransactions',\n",
    "            'UniqueInvoices', 'TotalRevenue', 'AvgOrderValue', 'StdOrderValue',\n",
    "            'MinOrderValue', 'MaxOrderValue', 'MedianOrderValue', 'TotalQuantity', \n",
    "            'AvgQuantity', 'StdQuantity', 'UniqueProducts', 'AvgUnitPrice', \n",
    "            'StdUnitPrice', 'YearsActive', 'MonthsActive', 'QuartersActive',\n",
    "            'PreferredDayOfWeek', 'PreferredHour', 'WeekendPurchaseRate',\n",
    "            'PreferredSeason'\n",
    "        ]\n",
    "        \n",
    "        # Calculate derived features\n",
    "        customer_features['CustomerLifespan'] = (\n",
    "            customer_features['LastPurchase'] - customer_features['FirstPurchase']\n",
    "        ).dt.days\n",
    "        \n",
    "        customer_features['DaysSinceLastPurchase'] = (\n",
    "            reference_date - customer_features['LastPurchase']\n",
    "        ).dt.days\n",
    "        \n",
    "        customer_features['AvgDaysBetweenPurchases'] = (\n",
    "            customer_features['CustomerLifespan'] / \n",
    "            (customer_features['TotalTransactions'] - 1).clip(lower=1)\n",
    "        )\n",
    "        \n",
    "        # Advanced behavioral features\n",
    "        customer_features['PurchaseFrequency'] = (\n",
    "            customer_features['TotalTransactions'] / \n",
    "            (customer_features['CustomerLifespan'] + 1).clip(lower=1)\n",
    "        )\n",
    "        \n",
    "        customer_features['PurchaseVelocity'] = (\n",
    "            customer_features['TotalTransactions'] / \n",
    "            customer_features['CustomerLifespan'].clip(lower=1)\n",
    "        )\n",
    "        \n",
    "        customer_features['ProductDiversityRatio'] = (\n",
    "            customer_features['UniqueProducts'] / customer_features['TotalTransactions']\n",
    "        )\n",
    "        \n",
    "        customer_features['SpendingConsistency'] = (\n",
    "            1 / (1 + customer_features['StdOrderValue'] / \n",
    "                 customer_features['AvgOrderValue'].clip(lower=0.01))\n",
    "        )\n",
    "        \n",
    "        customer_features['RevenuePerTransaction'] = (\n",
    "            customer_features['TotalRevenue'] / customer_features['TotalTransactions']\n",
    "        )\n",
    "        \n",
    "        customer_features['RevenueGrowthPotential'] = (\n",
    "            customer_features['MaxOrderValue'] / customer_features['AvgOrderValue']\n",
    "        )\n",
    "        \n",
    "        # RFM Analysis\n",
    "        customer_features['Recency'] = customer_features['DaysSinceLastPurchase']\n",
    "        customer_features['Frequency'] = customer_features['TotalTransactions']\n",
    "        customer_features['Monetary'] = customer_features['TotalRevenue']\n",
    "        \n",
    "        # RFM Scores with better handling of edge cases\n",
    "        try:\n",
    "            customer_features['RecencyScore'] = pd.qcut(\n",
    "                customer_features['Recency'], self.config.RFM_QUANTILES, \n",
    "                labels=[5,4,3,2,1], duplicates='drop'\n",
    "            )\n",
    "        except ValueError:\n",
    "            # Fallback for when there aren't enough unique values\n",
    "            customer_features['RecencyScore'] = pd.cut(\n",
    "                customer_features['Recency'], self.config.RFM_QUANTILES, \n",
    "                labels=[5,4,3,2,1], duplicates='drop'\n",
    "            )\n",
    "        \n",
    "        try:\n",
    "            customer_features['FrequencyScore'] = pd.qcut(\n",
    "                customer_features['Frequency'].rank(method='first'), \n",
    "                self.config.RFM_QUANTILES, labels=[1,2,3,4,5], duplicates='drop'\n",
    "            )\n",
    "        except ValueError:\n",
    "            customer_features['FrequencyScore'] = pd.cut(\n",
    "                customer_features['Frequency'], self.config.RFM_QUANTILES, \n",
    "                labels=[1,2,3,4,5], duplicates='drop'\n",
    "            )\n",
    "        \n",
    "        try:\n",
    "            customer_features['MonetaryScore'] = pd.qcut(\n",
    "                customer_features['Monetary'], self.config.RFM_QUANTILES, \n",
    "                labels=[1,2,3,4,5], duplicates='drop'\n",
    "            )\n",
    "        except ValueError:\n",
    "            customer_features['MonetaryScore'] = pd.cut(\n",
    "                customer_features['Monetary'], self.config.RFM_QUANTILES, \n",
    "                labels=[1,2,3,4,5], duplicates='drop'\n",
    "            )\n",
    "        \n",
    "        # Convert to numeric\n",
    "        customer_features['RecencyScore'] = pd.to_numeric(customer_features['RecencyScore'], errors='coerce')\n",
    "        customer_features['FrequencyScore'] = pd.to_numeric(customer_features['FrequencyScore'], errors='coerce')\n",
    "        customer_features['MonetaryScore'] = pd.to_numeric(customer_features['MonetaryScore'], errors='coerce')\n",
    "        \n",
    "        # Churn risk analysis\n",
    "        churn_threshold = customer_features['AvgDaysBetweenPurchases'] * self.config.CHURN_RISK_MULTIPLIER\n",
    "        customer_features['ChurnRisk'] = (\n",
    "            customer_features['DaysSinceLastPurchase'] > churn_threshold\n",
    "        ).astype(int)\n",
    "        \n",
    "        customer_features['ChurnProbability'] = np.minimum(\n",
    "            customer_features['DaysSinceLastPurchase'] / churn_threshold.clip(lower=1), 1.0\n",
    "        )\n",
    "        \n",
    "        # Seasonality features\n",
    "        season_encoder = LabelEncoder()\n",
    "        customer_features['SeasonEncoded'] = season_encoder.fit_transform(\n",
    "            customer_features['PreferredSeason'].astype(str)\n",
    "        )\n",
    "        \n",
    "        # Fill missing values\n",
    "        numeric_columns = customer_features.select_dtypes(include=[np.number]).columns\n",
    "        customer_features[numeric_columns] = customer_features[numeric_columns].fillna(0)\n",
    "        \n",
    "        logging.info(f\"Created features for {len(customer_features)} customers\")\n",
    "        return customer_features\n",
    "    \n",
    "    def prepare_target_variable(self, feature_data, target_data):\n",
    "        \"\"\"Create CLV target variable from future period\"\"\"\n",
    "        logging.info(\"Preparing target variable\")\n",
    "        \n",
    "        # Calculate future CLV from target period\n",
    "        future_clv = target_data.groupby('CustomerID')['TotalAmount'].sum().reset_index()\n",
    "        future_clv.columns = ['CustomerID', 'FutureCLV']\n",
    "        \n",
    "        # Get customers from feature period\n",
    "        feature_customers = feature_data['CustomerID'].unique()\n",
    "        \n",
    "        # Create features from historical data only\n",
    "        features = self.create_advanced_features(feature_data)\n",
    "        \n",
    "        # Merge with future CLV\n",
    "        clv_data = features.merge(future_clv, on='CustomerID', how='left')\n",
    "        clv_data['FutureCLV'] = clv_data['FutureCLV'].fillna(0)\n",
    "        \n",
    "        logging.info(f\"Customers with future purchases: {(clv_data['FutureCLV'] > 0).sum()}\")\n",
    "        logging.info(f\"Customers with no future purchases: {(clv_data['FutureCLV'] == 0).sum()}\")\n",
    "        \n",
    "        return clv_data\n",
    "    \n",
    "    def prepare_features_for_modeling(self, df):\n",
    "        \"\"\"Prepare features for machine learning\"\"\"\n",
    "        \n",
    "        # Select features for modeling\n",
    "        feature_cols = [\n",
    "            'TotalTransactions', 'UniqueInvoices', 'TotalRevenue', 'AvgOrderValue',\n",
    "            'StdOrderValue', 'MinOrderValue', 'MaxOrderValue', 'MedianOrderValue',\n",
    "            'TotalQuantity', 'AvgQuantity', 'StdQuantity', 'UniqueProducts',\n",
    "            'AvgUnitPrice', 'StdUnitPrice', 'CustomerLifespan', 'DaysSinceLastPurchase',\n",
    "            'AvgDaysBetweenPurchases', 'PurchaseFrequency', 'PurchaseVelocity',\n",
    "            'ProductDiversityRatio', 'SpendingConsistency', 'RevenuePerTransaction',\n",
    "            'RevenueGrowthPotential', 'YearsActive', 'MonthsActive', 'QuartersActive',\n",
    "            'PreferredDayOfWeek', 'PreferredHour', 'WeekendPurchaseRate',\n",
    "            'RecencyScore', 'FrequencyScore', 'MonetaryScore', 'ChurnRisk',\n",
    "            'ChurnProbability', 'SeasonEncoded'\n",
    "        ]\n",
    "        \n",
    "        # Ensure all features exist\n",
    "        available_features = [col for col in feature_cols if col in df.columns]\n",
    "        self.feature_names = available_features\n",
    "        \n",
    "        X = df[available_features].copy()\n",
    "        y = df['FutureCLV'].copy()\n",
    "        \n",
    "        # Handle missing values\n",
    "        X = X.fillna(X.median())\n",
    "        \n",
    "        # Replace infinite values\n",
    "        X = X.replace([np.inf, -np.inf], np.nan)\n",
    "        X = X.fillna(X.median())\n",
    "        \n",
    "        logging.info(f\"Feature matrix shape: {X.shape}\")\n",
    "        logging.info(f\"Target variable shape: {y.shape}\")\n",
    "        \n",
    "        return X, y\n",
    "    \n",
    "    def calculate_business_metrics(self, y_true, y_pred, customer_ids=None):\n",
    "        \"\"\"Calculate business-focused evaluation metrics\"\"\"\n",
    "        \n",
    "        # Standard regression metrics\n",
    "        mse = mean_squared_error(y_true, y_pred)\n",
    "        rmse = np.sqrt(mse)\n",
    "        mae = mean_absolute_error(y_true, y_pred)\n",
    "        r2 = r2_score(y_true, y_pred)\n",
    "        \n",
    "        # Business-focused metrics\n",
    "        ranking_corr = spearmanr(y_true, y_pred)[0] if len(y_true) > 1 else 0\n",
    "        \n",
    "        # Top customer identification precision\n",
    "        if len(y_true) > 10:\n",
    "            top_10_pct = max(1, int(len(y_true) * 0.1))\n",
    "            true_top_indices = y_true.argsort()[-top_10_pct:]\n",
    "            pred_top_indices = y_pred.argsort()[-top_10_pct:]\n",
    "            top_10_precision = len(set(true_top_indices) & set(pred_top_indices)) / top_10_pct\n",
    "        else:\n",
    "            top_10_precision = 0\n",
    "        \n",
    "        # MAPE (Mean Absolute Percentage Error) for non-zero values\n",
    "        non_zero_mask = y_true > 0\n",
    "        if non_zero_mask.sum() > 0:\n",
    "            mape = mean_absolute_percentage_error(y_true[non_zero_mask], y_pred[non_zero_mask])\n",
    "        else:\n",
    "            mape = np.inf\n",
    "        \n",
    "        return {\n",
    "            'MSE': mse,\n",
    "            'RMSE': rmse,\n",
    "            'MAE': mae,\n",
    "            'R2': r2,\n",
    "            'MAPE': mape,\n",
    "            'Ranking_Correlation': ranking_corr,\n",
    "            'Top_10_Precision': top_10_precision\n",
    "        }\n",
    "    \n",
    "    def create_model_pipeline(self):\n",
    "        \"\"\"Create advanced model pipeline with preprocessing\"\"\"\n",
    "        \n",
    "        # Define models with hyperparameter grids (reduced for faster training)\n",
    "        models = {\n",
    "            'Linear_Regression': {\n",
    "                'model': LinearRegression(),\n",
    "                'params': {},\n",
    "                'scale': True\n",
    "            },\n",
    "            'Ridge_Regression': {\n",
    "                'model': Ridge(),\n",
    "                'params': {'alpha': [0.1, 1.0, 10.0]},\n",
    "                'scale': True\n",
    "            },\n",
    "            'Random_Forest': {\n",
    "                'model': RandomForestRegressor(random_state=self.config.RANDOM_STATE, n_jobs=2),\n",
    "                'params': {\n",
    "                    'n_estimators': [100, 200],\n",
    "                    'max_depth': [10, 20, None],\n",
    "                    'min_samples_split': [2, 5],\n",
    "                    'min_samples_leaf': [1, 2]\n",
    "                },\n",
    "                'scale': False\n",
    "            },\n",
    "            'Gradient_Boosting': {\n",
    "                'model': GradientBoostingRegressor(random_state=self.config.RANDOM_STATE),\n",
    "                'params': {\n",
    "                    'n_estimators': [100, 200],\n",
    "                    'learning_rate': [0.05, 0.1],\n",
    "                    'max_depth': [3, 5]\n",
    "                },\n",
    "                'scale': False\n",
    "            },\n",
    "            'Extra_Trees': {\n",
    "                'model': ExtraTreesRegressor(random_state=self.config.RANDOM_STATE, n_jobs=2),\n",
    "                'params': {\n",
    "                    'n_estimators': [100, 200],\n",
    "                    'max_depth': [10, 20],\n",
    "                    'min_samples_split': [2, 5]\n",
    "                },\n",
    "                'scale': False\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        return models\n",
    "    \n",
    "    def train_and_evaluate_models(self, X_train, X_test, y_train, y_test):\n",
    "        \"\"\"Train multiple models with hyperparameter tuning\"\"\"\n",
    "        logging.info(\"Training and evaluating models with hyperparameter tuning\")\n",
    "        \n",
    "        models = self.create_model_pipeline()\n",
    "        results = []\n",
    "        \n",
    "        # Initialize scalers\n",
    "        standard_scaler = StandardScaler()\n",
    "        \n",
    "        for name, model_config in models.items():\n",
    "            logging.info(f\"Training {name}...\")\n",
    "            \n",
    "            try:\n",
    "                model = model_config['model']\n",
    "                params = model_config['params']\n",
    "                needs_scaling = model_config['scale']\n",
    "                \n",
    "                # Prepare data\n",
    "                if needs_scaling:\n",
    "                    X_train_processed = standard_scaler.fit_transform(X_train)\n",
    "                    X_test_processed = standard_scaler.transform(X_test)\n",
    "                else:\n",
    "                    X_train_processed = X_train\n",
    "                    X_test_processed = X_test\n",
    "                \n",
    "                # Hyperparameter tuning\n",
    "                if params:\n",
    "                    grid_search = GridSearchCV(\n",
    "                        model, params, cv=3,  # Reduced CV folds for speed\n",
    "                        scoring='r2', n_jobs=2, verbose=0\n",
    "                    )\n",
    "                    grid_search.fit(X_train_processed, y_train)\n",
    "                    best_model = grid_search.best_estimator_\n",
    "                    best_params = grid_search.best_params_\n",
    "                else:\n",
    "                    best_model = model\n",
    "                    best_model.fit(X_train_processed, y_train)\n",
    "                    best_params = {}\n",
    "                \n",
    "                # Make predictions\n",
    "                y_pred_train = best_model.predict(X_train_processed)\n",
    "                y_pred_test = best_model.predict(X_test_processed)\n",
    "                \n",
    "                # Calculate metrics\n",
    "                train_metrics = self.calculate_business_metrics(y_train, y_pred_train)\n",
    "                test_metrics = self.calculate_business_metrics(y_test, y_pred_test)\n",
    "                \n",
    "                # Cross-validation\n",
    "                cv_scores = cross_val_score(\n",
    "                    best_model, X_train_processed, y_train, \n",
    "                    cv=3, scoring='r2'\n",
    "                )\n",
    "                \n",
    "                # Store results\n",
    "                result = {\n",
    "                    'Model': name,\n",
    "                    'Best_Params': str(best_params),\n",
    "                    'Train_R2': train_metrics['R2'],\n",
    "                    'Test_R2': test_metrics['R2'],\n",
    "                    'Train_RMSE': train_metrics['RMSE'],\n",
    "                    'Test_RMSE': test_metrics['RMSE'],\n",
    "                    'Test_MAE': test_metrics['MAE'],\n",
    "                    'Test_MAPE': test_metrics['MAPE'] if test_metrics['MAPE'] != np.inf else 999,\n",
    "                    'Ranking_Correlation': test_metrics['Ranking_Correlation'],\n",
    "                    'Top_10_Precision': test_metrics['Top_10_Precision'],\n",
    "                    'CV_R2_Mean': cv_scores.mean(),\n",
    "                    'CV_R2_Std': cv_scores.std(),\n",
    "                    'Needs_Scaling': needs_scaling\n",
    "                }\n",
    "                \n",
    "                results.append(result)\n",
    "                \n",
    "                # Store trained model\n",
    "                self.models[name] = {\n",
    "                    'model': best_model,\n",
    "                    'scaler': standard_scaler if needs_scaling else None,\n",
    "                    'params': best_params\n",
    "                }\n",
    "                \n",
    "                logging.info(f\"Completed {name} - R2: {test_metrics['R2']:.4f}\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                logging.error(f\"Error training {name}: {e}\")\n",
    "                continue\n",
    "        \n",
    "        # Convert to DataFrame and find best model\n",
    "        results_df = pd.DataFrame(results)\n",
    "        if not results_df.empty:\n",
    "            best_idx = results_df['Test_R2'].idxmax()\n",
    "            best_model_name = results_df.loc[best_idx, 'Model']\n",
    "            self.best_model = self.models[best_model_name]\n",
    "            self.is_fitted = True\n",
    "            \n",
    "            logging.info(f\"Best model: {best_model_name} with R2: {results_df.loc[best_idx, 'Test_R2']:.4f}\")\n",
    "        \n",
    "        return results_df\n",
    "    \n",
    "    def analyze_feature_importance(self, model_name=None):\n",
    "        \"\"\"Analyze feature importance for tree-based models\"\"\"\n",
    "        \n",
    "        if model_name is None and self.models:\n",
    "            model_name = list(self.models.keys())[0]\n",
    "        \n",
    "        model_info = self.models.get(model_name)\n",
    "        if not model_info:\n",
    "            return None\n",
    "        \n",
    "        model = model_info['model']\n",
    "        \n",
    "        # Feature importance for tree-based models\n",
    "        if hasattr(model, 'feature_importances_'):\n",
    "            importance_df = pd.DataFrame({\n",
    "                'Feature': self.feature_names,\n",
    "                'Importance': model.feature_importances_\n",
    "            }).sort_values('Importance', ascending=False)\n",
    "            \n",
    "            return importance_df\n",
    "        \n",
    "        return None\n",
    "    \n",
    "    def predict_clv(self, X):\n",
    "        \"\"\"Make CLV predictions using the best model\"\"\"\n",
    "        \n",
    "        if not self.is_fitted:\n",
    "            raise ValueError(\"Model must be fitted before making predictions\")\n",
    "        \n",
    "        model_info = self.best_model\n",
    "        model = model_info['model']\n",
    "        scaler = model_info['scaler']\n",
    "        \n",
    "        # Prepare data\n",
    "        if scaler is not None:\n",
    "            X_processed = scaler.transform(X)\n",
    "        else:\n",
    "            X_processed = X\n",
    "        \n",
    "        predictions = model.predict(X_processed)\n",
    "        return predictions\n",
    "    \n",
    "    def segment_customers(self, predictions, percentiles=[20, 40, 60, 80]):\n",
    "        \"\"\"Segment customers based on predicted CLV\"\"\"\n",
    "        \n",
    "        segments = pd.cut(predictions, \n",
    "                         bins=np.percentile(predictions, [0] + percentiles + [100]),\n",
    "                         labels=['Low Value', 'Low-Medium', 'Medium', 'Medium-High', 'High Value'],\n",
    "                         include_lowest=True)\n",
    "        \n",
    "        return segments\n",
    "    \n",
    "    def generate_business_insights(self, X, y_true, y_pred, segments):\n",
    "        \"\"\"Generate comprehensive business insights\"\"\"\n",
    "        \n",
    "        insights = {}\n",
    "        \n",
    "        # Overall model performance\n",
    "        metrics = self.calculate_business_metrics(y_true, y_pred)\n",
    "        insights['model_performance'] = metrics\n",
    "        \n",
    "        # Segment analysis\n",
    "        segment_stats = pd.DataFrame({\n",
    "            'Segment': segments,\n",
    "            'Predicted_CLV': y_pred,\n",
    "            'Actual_CLV': y_true\n",
    "        }).groupby('Segment').agg({\n",
    "            'Predicted_CLV': ['count', 'mean', 'median', 'std'],\n",
    "            'Actual_CLV': ['mean', 'median', 'std']\n",
    "        }).round(2)\n",
    "        \n",
    "        insights['segment_analysis'] = segment_stats\n",
    "        \n",
    "        # Revenue impact\n",
    "        total_predicted_clv = y_pred.sum()\n",
    "        high_value_mask = segments == 'High Value'\n",
    "        high_value_clv = y_pred[high_value_mask].sum() if high_value_mask.sum() > 0 else 0\n",
    "        \n",
    "        insights['revenue_impact'] = {\n",
    "            'total_predicted_clv': total_predicted_clv,\n",
    "            'high_value_contribution': high_value_clv,\n",
    "            'high_value_percentage': (high_value_clv / total_predicted_clv * 100) if total_predicted_clv > 0 else 0\n",
    "        }\n",
    "        \n",
    "        return insights\n",
    "    \n",
    "    def save_model(self, filepath):\n",
    "        \"\"\"Save the trained model and preprocessing components\"\"\"\n",
    "        \n",
    "        if not self.is_fitted:\n",
    "            logging.warning(\"Model hasn't been fitted yet. Saving current state anyway.\")\n",
    "        \n",
    "        model_package = {\n",
    "            'models': self.models,\n",
    "            'best_model': self.best_model,\n",
    "            'feature_names': self.feature_names,\n",
    "            'model_performance': self.model_performance,\n",
    "            'config': self.config,\n",
    "            'is_fitted': self.is_fitted,\n",
    "            'scaler': self.scaler\n",
    "        }\n",
    "        \n",
    "        try:\n",
    "            # Save using joblib for better sklearn model serialization\n",
    "            joblib.dump(model_package, filepath)\n",
    "            logging.info(f\"Model saved successfully to {filepath}\")\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error saving model: {e}\")\n",
    "            raise\n",
    "    \n",
    "    def load_model(self, filepath):\n",
    "        \"\"\"Load a pre-trained model and preprocessing components\"\"\"\n",
    "        \n",
    "        try:\n",
    "            model_package = joblib.load(filepath)\n",
    "            \n",
    "            self.models = model_package['models']\n",
    "            self.best_model = model_package['best_model']\n",
    "            self.feature_names = model_package['feature_names']\n",
    "            self.model_performance = model_package['model_performance']\n",
    "            self.config = model_package['config']\n",
    "            self.is_fitted = model_package['is_fitted']\n",
    "            self.scaler = model_package.get('scaler', None)\n",
    "            \n",
    "            logging.info(f\"Model loaded successfully from {filepath}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error loading model: {e}\")\n",
    "            raise\n",
    "    \n",
    "    def run_full_pipeline(self, file_path, save_model_path=None):\n",
    "        \"\"\"Run the complete CLV prediction pipeline\"\"\"\n",
    "        \n",
    "        try:\n",
    "            # Load and clean data\n",
    "            df = self.load_and_clean_data(file_path)\n",
    "            \n",
    "            # Create temporal split\n",
    "            feature_data, target_data = self.create_temporal_split(df)\n",
    "            \n",
    "            # Prepare features and target\n",
    "            clv_data = self.prepare_target_variable(feature_data, target_data)\n",
    "            X, y = self.prepare_features_for_modeling(clv_data)\n",
    "            \n",
    "            # Split data for training\n",
    "            X_train, X_test, y_train, y_test = train_test_split(\n",
    "                X, y, test_size=self.config.TEST_SIZE, \n",
    "                random_state=self.config.RANDOM_STATE\n",
    "            )\n",
    "            \n",
    "            # Train models\n",
    "            results = self.train_and_evaluate_models(X_train, X_test, y_train, y_test)\n",
    "            \n",
    "            # Make predictions\n",
    "            y_pred = self.predict_clv(X_test)\n",
    "            \n",
    "            # Customer segmentation\n",
    "            segments = self.segment_customers(y_pred)\n",
    "            \n",
    "            # Generate insights\n",
    "            insights = self.generate_business_insights(X_test, y_test, y_pred, segments)\n",
    "            \n",
    "            # Save model if path provided\n",
    "            if save_model_path:\n",
    "                self.save_model(save_model_path)\n",
    "            \n",
    "            return {\n",
    "                'model_results': results,\n",
    "                'predictions': y_pred,\n",
    "                'segments': segments,\n",
    "                'insights': insights,\n",
    "                'feature_importance': self.analyze_feature_importance()\n",
    "            }\n",
    "            \n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error in pipeline: {e}\")\n",
    "            raise\n",
    "\n",
    "# Example usage function\n",
    "def main():\n",
    "    \"\"\"Main function to demonstrate CLV prediction\"\"\"\n",
    "    \n",
    "    # Initialize predictor\n",
    "    predictor = EnhancedCLVPredictor()\n",
    "    \n",
    "    # File path - try different variations\n",
    "    possible_files = [\n",
    "        'online retail.xlsx',\n",
    "        'online_retail.xlsx',\n",
    "        'OnlineRetail.xlsx',\n",
    "        'Online Retail.xlsx'\n",
    "    ]\n",
    "    \n",
    "    file_path = None\n",
    "    for file in possible_files:\n",
    "        if os.path.exists(file):\n",
    "            file_path = file\n",
    "            break\n",
    "    \n",
    "    if file_path is None:\n",
    "        print(\"Error: Excel file not found. Please ensure your file is named correctly.\")\n",
    "        print(\"Expected names: 'online retail.xlsx', 'online_retail.xlsx', etc.\")\n",
    "        return\n",
    "    \n",
    "    try:\n",
    "        # Run pipeline\n",
    "        results = predictor.run_full_pipeline(file_path, save_model_path='clv_model.pkl')\n",
    "        \n",
    "        # Display results\n",
    "        print(\"\\n=== MODEL RESULTS ===\")\n",
    "        print(results['model_results'])\n",
    "        \n",
    "        print(\"\\n=== FEATURE IMPORTANCE ===\")\n",
    "        if results['feature_importance'] is not None:\n",
    "            print(results['feature_importance'].head(10))\n",
    "        \n",
    "        print(\"\\n=== BUSINESS INSIGHTS ===\")\n",
    "        print(f\"Model Performance: {results['insights']['model_performance']}\")\n",
    "        print(f\"Revenue Impact: {results['insights']['revenue_impact']}\")\n",
    "        \n",
    "        # Plot results\n",
    "        plt.figure(figsize=(15, 10))\n",
    "        \n",
    "        # Model comparison\n",
    "        plt.subplot(2, 2, 1)\n",
    "        results['model_results'].plot(x='Model', y='Test_R2', kind='bar', ax=plt.gca())\n",
    "        plt.title('Model Performance Comparison')\n",
    "        plt.xticks(rotation=45)\n",
    "        \n",
    "        # Feature importance\n",
    "        if results['feature_importance'] is not None:\n",
    "            plt.subplot(2, 2, 2)\n",
    "            top_features = results['feature_importance'].head(10)\n",
    "            plt.barh(top_features['Feature'], top_features['Importance'])\n",
    "            plt.title('Top 10 Feature Importance')\n",
    "        \n",
    "        # Customer segments\n",
    "        plt.subplot(2, 2, 3)\n",
    "        segment_counts = pd.Series(results['segments']).value_counts()\n",
    "        segment_counts.plot(kind='pie', ax=plt.gca())\n",
    "        plt.title('Customer Segments Distribution')\n",
    "        \n",
    "        # Prediction vs Actual\n",
    "        plt.subplot(2, 2, 4)\n",
    "        plt.scatter(results['insights']['model_performance'], results['predictions'][:100])\n",
    "        plt.xlabel('Actual CLV')\n",
    "        plt.ylabel('Predicted CLV')\n",
    "        plt.title('Predictions vs Actual (Sample)')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error running pipeline: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba709fe6-038e-456e-95d2-d7669f6696d8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
